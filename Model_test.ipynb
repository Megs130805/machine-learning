{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Megs130805/machine-learning/blob/main/Model_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaZ1Kw9qv7iy",
        "outputId": "282643c7-22c3-400a-a168-5ca68b8bf1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/Colab Notebooks/plant_village_datatset/archive (7)/PlantVillage/\" /content/\n",
        "dataset_path = \"/content/PlantVillage/\""
      ],
      "metadata": {
        "id": "dkKqkO-mwAgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# ğŸŒ¿ PlantVillage: Model Comparison (5 Models)\n",
        "# ===============================\n",
        "\n",
        "!pip install tensorflow scikit-learn --quiet\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2, VGG16, ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, MaxPooling2D, Flatten, Input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "# ===============================\n",
        "# 1ï¸âƒ£ Load Dataset\n",
        "# ===============================\n",
        "dataset_path = \"/content/PlantVillage/\"\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "num_classes = train_gen.num_classes\n",
        "\n",
        "# ===============================\n",
        "# 2ï¸âƒ£ Helper Function for Timing\n",
        "# ===============================\n",
        "def timed_train(model, train_gen, val_gen, epochs=5):\n",
        "    start = time.time()\n",
        "    history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, verbose=1)\n",
        "    duration = time.time() - start\n",
        "    val_acc = history.history['val_accuracy'][-1]\n",
        "    return val_acc, duration\n",
        "\n",
        "results = {}\n",
        "\n",
        "# ===============================\n",
        "# 3ï¸âƒ£ Model 1 â€“ MobileNetV2 (Transfer Learning)\n",
        "# ===============================\n",
        "base_mobilenet = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "base_mobilenet.trainable = False\n",
        "\n",
        "x = base_mobilenet.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "pred = Dense(num_classes, activation='softmax')(x)\n",
        "model_mobilenet = Model(inputs=base_mobilenet.input, outputs=pred)\n",
        "\n",
        "model_mobilenet.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "acc_mob, time_mob = timed_train(model_mobilenet, train_gen, val_gen)\n",
        "results[\"MobileNetV2\"] = (acc_mob, time_mob)\n",
        "\n",
        "# ===============================\n",
        "# 4ï¸âƒ£ Model 2 â€“ Custom CNN (Simple)\n",
        "# ===============================\n",
        "model_cnn = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "acc_cnn, time_cnn = timed_train(model_cnn, train_gen, val_gen)\n",
        "results[\"Custom CNN\"] = (acc_cnn, time_cnn)\n",
        "\n",
        "# ===============================\n",
        "# 5ï¸âƒ£ Model 3 â€“ VGG16 (Frozen)\n",
        "# ===============================\n",
        "base_vgg = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "base_vgg.trainable = False\n",
        "\n",
        "x = base_vgg.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "pred = Dense(num_classes, activation='softmax')(x)\n",
        "model_vgg = Model(inputs=base_vgg.input, outputs=pred)\n",
        "\n",
        "model_vgg.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "acc_vgg, time_vgg = timed_train(model_vgg, train_gen, val_gen)\n",
        "results[\"VGG16\"] = (acc_vgg, time_vgg)\n",
        "\n",
        "# ===============================\n",
        "# 6ï¸âƒ£ Model 4 â€“ ResNet50 (Frozen)\n",
        "# ===============================\n",
        "base_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "base_resnet.trainable = False\n",
        "\n",
        "x = base_resnet.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "pred = Dense(num_classes, activation='softmax')(x)\n",
        "model_resnet = Model(inputs=base_resnet.input, outputs=pred)\n",
        "\n",
        "model_resnet.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "acc_resnet, time_resnet = timed_train(model_resnet, train_gen, val_gen)\n",
        "results[\"ResNet50\"] = (acc_resnet, time_resnet)\n",
        "\n",
        "# ===============================\n",
        "# 7ï¸âƒ£ Model 5 â€“ SVM (on Flattened Images)\n",
        "# ===============================\n",
        "print(\"\\nExtracting small subset for SVM (this might take a while)...\")\n",
        "\n",
        "# Smaller subset for CPU\n",
        "X, y = [], []\n",
        "for i in range(500):  # only first 500 images for speed\n",
        "    img, label = train_gen.next()\n",
        "    X.append(img[0].flatten())\n",
        "    y.append(np.argmax(label[0]))\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "svm = SVC(kernel='rbf')\n",
        "start = time.time()\n",
        "svm.fit(X, y)\n",
        "duration_svm = time.time() - start\n",
        "acc_svm = svm.score(X, y)\n",
        "results[\"SVM (raw pixels)\"] = (acc_svm, duration_svm)\n",
        "\n",
        "# ===============================\n",
        "# 8ï¸âƒ£ Comparison Table\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results, index=[\"Validation Accuracy\", \"Training Time (sec)\"]).T\n",
        "df[\"Validation Accuracy\"] = (df[\"Validation Accuracy\"] * 100).round(2)\n",
        "df[\"Training Time (min)\"] = (df[\"Training Time (sec)\"] / 60).round(2)\n",
        "df.drop(columns=[\"Training Time (sec)\"], inplace=True)\n",
        "print(\"\\n================= MODEL PERFORMANCE COMPARISON =================\\n\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KcJF3MtBwp3X",
        "outputId": "815a9811-c16e-4215-d346-3461e738306e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16516 images belonging to 15 classes.\n",
            "Found 4122 images belonging to 15 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 115ms/step - accuracy: 0.6560 - loss: 1.1216 - val_accuracy: 0.8806 - val_loss: 0.3598\n",
            "Epoch 2/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 70ms/step - accuracy: 0.8655 - loss: 0.4149 - val_accuracy: 0.8950 - val_loss: 0.3052\n",
            "Epoch 3/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 61ms/step - accuracy: 0.8971 - loss: 0.3161 - val_accuracy: 0.9064 - val_loss: 0.2779\n",
            "Epoch 4/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 61ms/step - accuracy: 0.9166 - loss: 0.2519 - val_accuracy: 0.9192 - val_loss: 0.2452\n",
            "Epoch 5/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 62ms/step - accuracy: 0.9220 - loss: 0.2279 - val_accuracy: 0.9199 - val_loss: 0.2341\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 81ms/step - accuracy: 0.4218 - loss: 1.8944 - val_accuracy: 0.7785 - val_loss: 0.6762\n",
            "Epoch 2/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 69ms/step - accuracy: 0.7543 - loss: 0.7371 - val_accuracy: 0.8508 - val_loss: 0.4448\n",
            "Epoch 3/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 77ms/step - accuracy: 0.8412 - loss: 0.4786 - val_accuracy: 0.8797 - val_loss: 0.3678\n",
            "Epoch 4/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 69ms/step - accuracy: 0.8822 - loss: 0.3503 - val_accuracy: 0.8845 - val_loss: 0.3602\n",
            "Epoch 5/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 73ms/step - accuracy: 0.9149 - loss: 0.2565 - val_accuracy: 0.8806 - val_loss: 0.3718\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 232ms/step - accuracy: 0.3655 - loss: 2.0494 - val_accuracy: 0.7213 - val_loss: 1.0049\n",
            "Epoch 2/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 203ms/step - accuracy: 0.6791 - loss: 1.0366 - val_accuracy: 0.7863 - val_loss: 0.7182\n",
            "Epoch 3/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 203ms/step - accuracy: 0.7520 - loss: 0.7902 - val_accuracy: 0.8147 - val_loss: 0.5954\n",
            "Epoch 4/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 203ms/step - accuracy: 0.7841 - loss: 0.6828 - val_accuracy: 0.8190 - val_loss: 0.5602\n",
            "Epoch 5/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 203ms/step - accuracy: 0.8041 - loss: 0.6081 - val_accuracy: 0.8450 - val_loss: 0.4811\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 129ms/step - accuracy: 0.1666 - loss: 2.5450 - val_accuracy: 0.2715 - val_loss: 2.2643\n",
            "Epoch 2/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 104ms/step - accuracy: 0.2391 - loss: 2.2893 - val_accuracy: 0.2974 - val_loss: 2.0936\n",
            "Epoch 3/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 103ms/step - accuracy: 0.2761 - loss: 2.1378 - val_accuracy: 0.3428 - val_loss: 1.9751\n",
            "Epoch 4/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 104ms/step - accuracy: 0.3143 - loss: 2.0409 - val_accuracy: 0.3307 - val_loss: 1.8832\n",
            "Epoch 5/5\n",
            "\u001b[1m517/517\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 104ms/step - accuracy: 0.3325 - loss: 1.9747 - val_accuracy: 0.4068 - val_loss: 1.8189\n",
            "\n",
            "Extracting small subset for SVM (this might take a while)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DirectoryIterator' object has no attribute 'next'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3959441580.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# only first 500 images for speed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'next'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# ğŸ§  Run SVM Separately & Add to Results\n",
        "# ===============================\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "print(\"\\nRunning SVM separately (subset of training images)...\")\n",
        "\n",
        "X, y = [], []\n",
        "subset_size = 500  # can increase if system can handle more\n",
        "train_gen.reset()  # reset generator to start from beginning\n",
        "\n",
        "for i in range(subset_size):\n",
        "    img, label = next(train_gen)\n",
        "    X.append(img[0].flatten())\n",
        "    y.append(np.argmax(label[0]))\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "print(\"Training SVM...\")\n",
        "svm = SVC(kernel='rbf')\n",
        "start = time.time()\n",
        "svm.fit(X, y)\n",
        "duration_svm = time.time() - start\n",
        "acc_svm = svm.score(X, y)\n",
        "\n",
        "# Save results without losing previous ones\n",
        "results[\"SVM (raw pixels)\"] = (acc_svm, duration_svm)\n",
        "\n",
        "# ===============================\n",
        "# ğŸ” Show Updated Comparison Table\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results, index=[\"Validation Accuracy\", \"Training Time (sec)\"]).T\n",
        "df[\"Validation Accuracy\"] = (df[\"Validation Accuracy\"] * 100).round(2)\n",
        "df[\"Training Time (min)\"] = (df[\"Training Time (sec)\"] / 60).round(2)\n",
        "df.drop(columns=[\"Training Time (sec)\"], inplace=True)\n",
        "\n",
        "print(\"\\n================= UPDATED MODEL PERFORMANCE COMPARISON =================\\n\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiK31NQQQmhv",
        "outputId": "70c3a5e0-7319-4192-d07f-9fa7336bffdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running SVM separately (subset of training images)...\n",
            "Training SVM...\n",
            "\n",
            "================= UPDATED MODEL PERFORMANCE COMPARISON =================\n",
            "\n",
            "                  Validation Accuracy  Training Time (min)\n",
            "MobileNetV2                     91.99                 3.49\n",
            "Custom CNN                      88.06                 3.30\n",
            "VGG16                           84.50                 9.23\n",
            "ResNet50                        40.68                 4.91\n",
            "SVM (raw pixels)                91.80                 1.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BPyUWCI7P9Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Available devices:\")\n",
        "print(tf.config.list_physical_devices())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTgp-NyHK5u1",
        "outputId": "d7cc9f3e-e167-4bbf-981b-cac9864c67b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available devices:\n",
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    }
  ]
}